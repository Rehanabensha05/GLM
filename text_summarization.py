# -*- coding: utf-8 -*-
"""text_summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_U41p1odaLdwHWATNTasEGjvjlonD8yC
"""

#import packages
import requests
from bs4 import BeautifulSoup
import nltk
nltk.download('punkt')
#word_tokenization
from nltk.tokenize import word_tokenize
#sentence_tokenization
from nltk.tokenize import sent_tokenize
#Data manipulation
import pandas as pd
#data pre_processing
import re
#stop_words_removal
from nltk.corpus import stopwords
nltk.download('stopwords')
#word_frequency
from collections import defaultdict
import string
from string import punctuation
from nltk.corpus.reader import WordListCorpusReader
from textblob import TextBlob
from heapq  import nlargest

#make a request
data=requests.get("https://en.wikipedia.org/wiki/Telecommunications")

#html format
soup1=BeautifulSoup(data.content,'html.parser')

soup1

#extract paragraph
para1=soup1.find_all('p')
for p1 in para1:
    print(p1.get_text())

text=[p.text for p in para1]
new_data=pd.DataFrame({"text1":text})

new_data.head()

#data preprocessing
new_data['text1']=new_data['text1'].apply(lambda x: re.sub(r'[^\w\s]','',x))
new_data['text1']=new_data['text1'].apply(lambda x: re.sub(r'\+d','',x))
new_data['text1']=new_data['text1'].apply(lambda x: re.sub(r'http\S+','',x))
new_data.head()

#attributes
new_data.shape

#data_type
new_data.info()

#data cleaning
stop_words = set(stopwords.words('english'))
new_data['word_token']=new_data['text1'].apply(word_tokenize)
new_data = new_data['word_token'].apply(lambda x: [word for word in x if word not in stop_words])
new_data.head()

data=pd.Series(new_data)
data

punctuation = punctuation + '\n'
punctuation

word_frequencies = defaultdict(int)

# Iterate through each sublist in the pandas Series
for sublist in new_data:
    # Check if the sublist is actually a list
    if isinstance(sublist, list):
        # Iterate through each word in the sublist
        for word in sublist:
            # Check if the word is a string
            if isinstance(word, str):
                word_text = word.lower()

                word_frequencies[word_text] += 1

# Print the word frequencies
print(word_frequencies)

max_frequencies=max(word_frequencies.values())

max_frequencies

# Apply sentence tokenization
# Summary generator
def generate_summary(text, n):
  new_data['tokenized_sentences'] = new_data['text1'].apply(sent_tokenize)

new_data['tokenized_sentences']

#sentence score
def score_sentences(sentences):
    scores = []
    for sentence in sentences:
        blob = TextBlob(sentence)
        sentiment_score = blob.sentiment.polarity  # Sentiment polarity score (-1 to 1)
        scores.append(sentiment_score)
    return scores

# Apply the scoring function to each list of sentences
new_data['sentence_scores'] = new_data['tokenized_sentences'].apply(score_sentences)

# Display the DataFrame with sentence scores
print(new_data[['tokenized_sentences', 'sentence_scores']])

select_length = int(len(new_data['sentence_scores'])*0.3)
select_length

summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)

summary = generate_summary(data, 20)
#summary_sentences = summary.split('. ')
formatted_summary = '.\n'.join(new_data['text1'])

print(formatted_summary)

final_summary = [word.text for word in summary]


summary = ' '.join(final_summary)


print(text)

# Evaluation
!pip install rouge-score
from rouge_score import rouge_scorer
import rouge
from rouge import Rouge
def evaluate_rouge(final_summary, summary_text):
  rouge = Rouge()
  scores = rouge.get_scores(final_summary, summary_text)
  return scores[0]['rouge-1']['f']

def generate_summary(text, num_sentences):
    # Dummy implementation for illustration
    return " ".join(text[:num_sentences])  # Replace with actual summarization logic

from rouge_score import rouge_scorer

def evaluate_rouge(reference_summary, generated_summary):
    # Initialize the ROUGE scorer
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

    # Compute the ROUGE scores
    scores = scorer.score(reference_summary, generated_summary)

    return scores

summary = generate_summary(new_data, 20)

# Evaluate the summary using ROUGE
rouge_score = evaluate_rouge(reference_summary, summary)

print(f"ROUGE score: {rouge_score}")

#Initialize tokenizer and model
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large')
model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large')


# Tokenize and generate summary
combined_text = " ".join(new_data['text1'].astype(str).tolist())
input_ids = tokenizer.encode(combined_text, return_tensors='pt', max_length=512, truncation=True)
output = model.generate(input_ids, max_length=150, num_beams=7, early_stopping=True)
output_text = tokenizer.decode(output[0], skip_special_tokens=True)

print(output_text)

summary = generate_summary(new_data, 20)

# Evaluate the summary using ROUGE
rouge_score = evaluate_rouge(output_text, summary)

print(f"ROUGE score: {rouge_score}")